#同步和排序约束

虽然不能配置原子数据类型的原子性，但可以调整原子操作的同步和排序约束。这在C#或Java的内存模型中是不可能的，只有在C++中可行。

C++中有六种不同的内存模型。那这些内存模型分别是什么呢?

## C++中六种内存序

我们已经知道C++有六种不同的内存序。原子操作默认的内存序是`std::memory_order_seq_cst`，这表示顺序的一致性。此外，也可以显式地指定其他五个中的一个。那么剩余几个是什么呢?

C++中定义的内存序

```c++
enum memory_order{
  memory_order_relaxed,
  memory_order_consume,
  memory_order_acquire,
  memory_order_release,
  memory_order_acq_rel,
  memory_order_seq_cst
}
```

要对这六种内存序进行分类，需要回答两个问题:

1. 不同的原子操作应该使用哪种内存模型?
2. 6个内存序定义了哪些同步和排序约束?

My plan is quite simple: I answer both questions.

我的计划很简单 —— 回答这两个问题。

## 原子操作的种类

这里有三种不同类型的原子操作：

* *读*(read)操作: `memory_order_acquire`和`memory_order_consume`
* *写*(write)操作: `memory_order_release`
* *读改写*(read-modify-write)操作: `memory_order_acq_rel`和`memory_order_seq_cst`

`memory_order_relaxed`无同步和排序约束，所以它不适用于这种分类方式。

下表根据原子操作的读写特性对它们进行排序。

|        操作名称         | read | write | read-modify-write |
| :---------------------: | :--: | :---: | :---------------: |
|      test_and_set       |      |       |        yes        |
|          clear          |      |  yes  |                   |
|      is_lock_free       | yes  |       |                   |
|          load           | yes  |       |                   |
|          store          |      |  yes  |                   |
|        exchange         |      |       |        yes        |
| compare_exchange_strong |      |       |        yes        |
|  compare_exchange_weak  |      |       |                   |
|      fetch_add, +=      |      |       |        yes        |
|      fetch_sub, -=      |      |       |                   |
|      fetch_or, \|=      |      |       |        yes        |
|      fetch_and, &=      |      |       |                   |
|      fetch_xor, ^=      |      |       |                   |
|         ++, --          |      |       |        yes        |

“读改写”操作还有一个额外的保证：总是提供最新的值。这意味着，不同线程上的`atomVar.fetch_sub(1)`操作序列一个接一个地计数，无缝衔接或进行重复。

如果将原子操作`atomVar.load()`与“写”或“读改写”操作一起使用，那么“写”的部分将不起作用。结果就是：`atomVar.load(std::memory_order_acq_rel)`等价于`atomVar.load(std::memory_order_acquire)`，`atomVar.load(std::memory_order_release)`等价于`atomVar.load(std::memory_order_relax)`。

## 同步与排序约束的不同

大致说来，C++中有三种不同类型的同步和排序约束:

* 顺序一致性: `memory_order_seq_cst`
* 获取-释放(Acquire-release)：`memory_order_consume` , `memory_order_acquire` ,` memory_order_release`和`memory_order_acq_rel`
* 自由序(Relaxed): `memory_order_relaxed`

顺序一致性在线程之间建立全局顺序，而获取-释放语义为不同线程之间，对同一原子变量进行读写操作时建立顺序。自由语序只保证了原子变量的修改顺序，修改顺序是指对一个特定原子变量的所有修改都以某种特定的顺序发生。因此，由特定线程读取原子对象时，不会看到比线程已经观察到的值“更旧”的值。

不同的内存模型，及其对原子和非原子操作的影响，使建立C++内存模型有趣但又具有挑战性。下面我们来讨论顺序一致性、获得-释放语义和自由语义的同步和排序约束。

### 顺序一致性

让我们深入地研究一下顺序一致性，其关键是所有线程上的所有操作都遵从一个通用时钟。这个全球时钟让我们可以很直观的想象它的存在。

顺序一致性的直观性是有代价的，缺点是系统必须对线程进行同步。

下面的程序在顺序一致性的帮助下，同步生产者和消费者线程。

```c++
// producerConsumer.cpp

#include <atomic>
#include <iostream>
#include <string>
#include <thread>

std::string work;
std::atomic<bool> ready(false);

void consumer(){
  while(!ready.load()){}
  std::cout << work << std::endl;
}

void producer(){
  work = "done";
  ready = true;
}

int main(){
  std::thread prod(producer);
  std::thread con(consumer);
  prod.join();
  con.join();
}
```

这个程序的输出：

![](E:/openSourceProjecrts/gitbook/Concurrency-with-Modern-C++/images/detail/memory-model/9.png)

由于顺序一致性，程序执行结果是确定的，所以总是输出“done”。

The graphic depicts the sequence of operations. The consumer thread waits in the while-loop until the atomic variable ready is set to true . When this happens, the consumer threads continues its work.

下图描述了操作的顺序。消费者线程在`while`循环中等待，直到原子变量`ready`被生产者线程设置为`true`。当这种情况发生时，消费者线程将继续其工作。

![](E:/openSourceProjecrts/gitbook/Concurrency-with-Modern-C++/images/detail/memory-model/10.png)

理解程序总是返回“done”并不困难，只需要使用顺序一致性的两个特性：一方面，两个线程以源代码的顺序执行指令；另一方面，每个线程以相同的顺序查看另一个线程的操作。两个线程遵循相同的时钟。在`while(!ready.load()){}`循环中，这种同步也可以保持下去——用于同步生产者线程和消费者线程。

通过使用内存序，可以更正式地解释这个过程。以下是正式版本:

1. `work= "done"` 在序列中，位于 `ready = true`之前
   ⇒ `work= "done"` 先行与 `ready = true`
2. `while(!ready.load()){}`序列位位于 `std::cout << work << std::endl`之前
   ⇒ `while(!ready.load()){}`先行与`std::cout<< work << std::endl`
3. `ready= true`与`while(!ready.load()){}`同步
   ⇒ `ready= true`(线程间)先行于 `while (!ready.load()){}`
   ⇒ `ready= true`先行于`while (!ready.load()){}`

最终的结论：因为先行关系是可以传递的，所以`work = "done"`先行于`ready= true`，且先行于`while(!ready.load()){}`，更先行于`std::cout<< work << std::endl`。

顺序一致性中，一个线程可以看到另一个线程的操作，因此也可以看到所有其他线程的操作。如果使用原子操作的获取-释放语义，那么顺序一致性就不成立了。这是与C#和Java不同的地方，也是我们开始产生疑惑的地方。

### 获取-释放 语义

There is no global synchronisation between threads in the acquire-release semantic; there is only synchronisation between atomic operations on the same atomic variable. A write operation on one thread synchronises with a read operation on another thread on the same atomic variable.

The acquire-release semantic is based on one fundamental idea: a release operation synchronises with an acquire operation on the same atomic and establishes an ordering constraint. This means all read and write operations cannot be moved after a release operation, and all read and write operations cannot be moved before an acquire operation.

What is an acquire or release operation? The reading of an atomic variable with load or test_and_set is an acquire operation. There is more: There is more: the releasing of a lock or mutex synchronizes-with the acquiring of a lock or a mutex. The construction of a thread synchronizes-with the invocation of the callable. The completion of the thread synchronizes-with the join-call. The completion of the callable of the task synchronizes-with the call to wait or get on the future. Acquire and release operations come in pairs.

It helps a lot to keep that picture in mind.

![](E:/openSourceProjecrts/gitbook/Concurrency-with-Modern-C++/images/detail/memory-model/11.png)

> 贴士
>
> **The memory model for a deeper understanding of mul-tithreading**
>
> This is the main reason you should keep the memory model in mind. In particular the acquire-release semantic helps you to get a better understanding of the high-level synchronisation primitives such as a mutex. The same reasoning holds for the starting of a thread and the join-call on a thread. Both are acquire-release operations. The story goes on with the wait and notify_one call on a condition variable; wait is the acquire and notify_one the release operation. What’s about notify_all ? That is a release operation as well.

Now, let us look once more at the spinlock in the subsection std::atomic_flag. We can write it more efficiently because the synchronisation is done with the atomic_flag flag . Therefore the acquire-release semantic applies.

A Spinlock with acquire-release semantic



The flag.clear call in line 16 is a release, the flag.test_and_set call in line 12 an acquire operation, and the acquire synchronises with the release operation. The heavyweight synchronization of two threads with sequential consistency ( std::memory_order_seq_cst ) is replaced by the more lightweight and performant acquire-release semantic ( std::memory_order_acquire and std::memory_order_release ). The behaviour is not affected.

Although the flag.test_and_set(std::memory_order_acquire) call is a read-modify-write operation, the acquire semantic is sufficient. In summary, flag is an atomic and guarantees, therefore, modification order. This means all modifications to flag occur in some particular total order.

The acquire-release semantic is transitive. That means if you have an acquire-release semantic between two threads(a,b) and an acquire-release semantic between(b,c), you get an acquire-release semantic between (a, c).

## 传递性

A release operation synchronises with an acquire operation on the same atomic variable and, additionally, establishes ordering constraint. These are the components to synchronise threads in a performant way if they act on the same atomic. How can that work if two threads share no atomic variable? We do not want any sequential consistency because that is too expensive, but we want the light-weight acquire-release semantic.

The answer to this question is straightforward. Applying the transitivity of the acquire-release semantic, we can synchronise threads that are independent.

In the following example, thread t2 with its work package deliveryBoy is the connection between two independent threads t1 and t3 

Transitivity of the acquire-release semantics



The output of the program is deterministic. mySharedWork has the values 1,2 and 3.

![](E:/openSourceProjecrts/gitbook/Concurrency-with-Modern-C++/images/detail/memory-model/12.png)

There are two important observations:

1. Thread t2 waits in line 18, until thread t3 sets dataProduced to true (line 14).
2.  Thread t1 waits in line 23, until thread t2 sets dataConsumed to true (line 19).

Let me explain the rest with a graphic.

![](E:/openSourceProjecrts/gitbook/Concurrency-with-Modern-C++/images/detail/memory-model/13.png)

> Transitivity of the acquire-release semantic

The essential parts of the picture are the arrows.

* The blue arrows are the sequenced-before relations. This means that all operations in one thread are executed in source code order.
* The red arrows are the synchronizes-with relations. The reason is the acquire-release semantic of the atomic operations on the same atomic. The synchronisation between the atomics, and therefore between the threads happen at specific points.
* sequenced-before establishes a happens-before and synchronizes-with a inter-thread happens-before relation.

The rest is pretty simple. The happens-before and inter-thread happens-before order of the instructions corresponds to the direction of the arrows from top to bottom. Finally, we have the guarantee that mySharedWork[1] == 2 is executed last.

A release operation synchronizes-with an acquire operation on the same atomic variable, so we can easily synchronise threads, if … . The typical misunderstanding is about the if.

## 典型的误解

What is my motivation for writing about the typical misunderstanding of the acquire-release semantic? Many of my readers and students have already fallen into this trap. Let’s look at the straightforward case.

### Waiting Included

Here is a simple program as a starting point.

Acquire-release with waiting

```c++

```

The consumer thread t1 in line 17 waits until the consumer thread t2 in line 13 sets dataProduced to true . dataProduced is the guard and it guarantees that access to the non-atomic variable mySharedWork is synchronised. This means that the producer thread t2 initialises mySharedWork then the consumer thread t2 finishes the work by setting mySharedWork[1] to 2 . The program is well-defined.

![](E:/openSourceProjecrts/gitbook/Concurrency-with-Modern-C++/images/detail/memory-model/14.png)

The graphic shows the happens -before relation with in the threads and the synchronizes-with relation between the threads. synchronizes-with establishes an inter-thread happens-before relation. The rest of the reasoning is the transitivity of the happens-before relation.

Finally it holds that mySharedWork = {1, 0, 3} happens-before mySharedWork[1] = 2 

![](E:/openSourceProjecrts/gitbook/Concurrency-with-Modern-C++/images/detail/memory-model/15.png)

What aspect is often missing in this reasoning? The if.

### If …

What happens if the consumer thread t1 in line 17 doesn’t wait for the producer thread t2 ?

Acquire-release without waiting

```c++

```

The program has undefined behaviour because there is a data race on the variable mySharedWork . When we let the program run, we get the following non-deterministic behaviour.

![](E:/openSourceProjecrts/gitbook/Concurrency-with-Modern-C++/images/detail/memory-model/16.png)

What is the issue? It holds that dataProduced.store(true, std::memory_order_release) synchronizes-with dataProduced.load(std::memory_order_acquire) . But that doesn’t mean the acquire operation waits for the release operation, and that is exactly what is displayed in the graphic. In the graphic the dataProduced.load(std::memory_order_acquire) instruction is performed before the instruction dataProduced.store(true, std::memory_order_release) . We have no synchronizes-with relation.

![](../../../images/detail/memory-model/17.png)

#### 解决办法

synchronizes-with means: if dataProduced.store(true, std::memory_order_release) happens before dataProduced.load(std::memory_order_acquire) , then all visible effects of the operations before dataProduced.store(true, std::memory_order_release) are visible after dataProduced.load(std::memory_order_acquire) .The key is the word if. That if is guaranteed in the first program with the predicate (while(!dataProduced.load(std::memory_order_acquire)) 

Once again, but more formally.

All operations before dataProduced.store(true, std::memory_order_release) happens-before all operations after dataProduced.load(std::memory_order_acquire) , if the following holds : dataProduced.store(true, std::memory_order_release) happens-before dataProduced.load(std::memory_order_acquire) 

#### 释放顺序

A release sequence is a quite advanced concept when dealing with acquire-release semantic. So let first start with the acquire-release semantic in the following example.

Acquire-release without waiting

```c++

```

Left first look at the example without thread t3 . The atomic store on line 15 synchronizes-with the atomic load in line 19. The synchronisation guarantees that all that happens before the store is available after the load. This means in particular that the access to the non-atomic variable somethingShared is not a data race.

What changes if I use the thread t3 ? Now there seems to be a data race. As I already mentioned, the first call to atom.fetch_sub(1, std::memory_order_acquire) (line 19) has an acquire-release semanticwith atom.store(2, std::memory_order_release (line15);therefore, their is no data race on somethingShared .

This does not hold for the second call to atom.fetch_sub(1, std::memory_order_acquire) . It is a ready-modify-write operation without a std::memory_order_release tag. This means in particular that the second call to atom.fetch_sub(1, std::memory_order_acquire) does not synchronize-
with the first call and a data race may occur on sharedVariable . May because thanks to the release sequence this does not happen. The release sequence is extended to the second call to atom.fetch_sub(1, std::memory_order_acquire) ;therefore,the second call atom.fetch_sub(1, std::memory_order_acquire) has a happens-before relation with the first call.

Finally, here is the output of the program.

![](../../../images/detail/memory-model/18.png)

More formally the definition of a release sequence by the [N4659: Working Draft, Standard for Programming Language C++]( http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/n4659.pdf).

> 知识点
>
> **释放顺序**
>
> A release sequence headed by a release operation A on an atomic object M is a maximal contiguous sub-sequence of side effects in the modification order of M, where the first operation is A, and every subsequent operation * is performed by the same thread that performed A, or * is an atomic read-modify-write operation.

If you carefully follow my explanation such as the one in the subsection Challenges, you probably expect Relaxed Semantic to come next; however I’ll look first at the memory model std::memory_order_consume which is quite similar to std::memory_order_acquire 

#### std::memory_order_consume

std::memory_order_consume is the most legendary of the six memory orderings. That is for two reasons: first, std::memory_order_consume is extremely hard to understand and second - and this may change in the future - no compiler supports it currently. With C++17 the situation gets even worse. Here is the official wording: “The specification of release-consume ordering is being revised, and the use of memory_order_consume is temporarily discouraged.”

How can it be that a compiler that implements the C++11 standard doesn’t support the memory model std::memory_order_consume ? The answer is that the compiler maps std::memory_order_consume to std::memory_order_acquire . This is acceptable because both are load or acquire operations. std::memory_order_consume requires weaker synchronisation and ordering constraints than std::memory_order_acquire .Therefore, the release-acquire ordering is potentially slower than the release-consume ordering but - and this is the key point - well-defined.

To get an understanding of the release-consume ordering, it is a good idea to compare it with the release-acquire ordering. I speak in the following subsection explicitly about the release-acquire ordering and not about the acquire-release semantic to emphasise the strong relationship of std::memory_order_consume and std::memory_order_acquire .

#### Release-acquire排序

As a starting point, let us use the following program with two threads t1 and t2 . t1 plays the role of the producer, t2 the role of the consumer. The atomic variable ptr helps to synchronise the producer and consumer.

Release-acquire ordering

```c++

```

Before analysing the program, I want to introduce a small variation.

#### Release-consume排序

I replace the memory order std::memory_order_acquire in line 21 with std::memory_order_consume 

Release-acquire ordering

```c++

```

Now the program has undefined behaviour. This statement is very hypothetical because my GCC 5.4 compiler implements std::memory_order_consume byusing std::memory_order_acquire .Under the hood, both programs do the same thing.

#### Release-acquire versus Release-consume ordering

The outputs of the programs are identical.

![](../../../images/detail/memory-model/19.png)

At the risk of repeating myself, I want to add a few words explaining why the first program acquireRelease.cpp is well-defined.

The store operation on line 16 synchronizes-with the load operation in line 21. The reason is that the store operation uses std::memory_order_release and the load operation uses std::memory_order_acquire . This is the synchronisation. What are the ordering constraints for the release-acquire
operations? The release-acquire ordering guarantees that the results of all operations before the store operation (line 16) are available after the load operation (line 21). So also, the release-acquire
operation orders the access to the non-atomic variable data (line14) and the atomic variable atoData (line 15). That holds, although atoData uses the std::memory_order_relaxed memory ordering.

The crucial question is: what happens if I replace std::memory_order_acquire with std::memory_order_consume ?

#### Data dependencies with std::memory_order_consume

std::memory_order_consume deals with data dependencies on atomics. Data dependencies exist in two ways. First, let us look at carries-a-dependency-to in a thread and dependency-ordered before between two threads. Both dependencies introduce a happens-before relation. These are the kind of relations we are looking for. What does carries-a-dependency-to and dependency-order-before mean?

* carries-a-dependency-to: if the result of operation A is used as an operand in operation B, then: A carries-a-dependency-to B.
* dependency-ordered-before: a store operation (with std::memory_order_release, std::memory_order_acq_rel, or std::memory_order_seq_cst) is dependency-ordered-before a load operation B (with std::memory_order_consume) if the result of load operation B is used in a further operation C in the same thread. It is important to note that operations B and C have to be in the same thread.

I know from personal experience that both definitions might not be easy to digest. Here is a graphic to visualise them.

![](../../../images/detail/memory-model/20.png)

The expression ptr.store(p, std::memory_order_release) is dependency-ordered-before the expression while (!(p2 = ptr.load(std::memory_order_consume))) , because the following line std::cout << "*p2: " << *p2 << std::endl is be read as the result of the load operation. Furthermore it holds that: while (!(p2 = ptr.load(std::memory_order_consume)) carries-a-dependency-to std::cout << "*p2: " << *p2 << std::endl , because the output of *p2 uses the result of the ptr.load operation.

We have no guarantee regarding the output of data and atoData . That’s because neither has a carries-a-dependency relation to the ptr.load operation. It gets even worse: since data is a non-atomic variable, there is a race condition on the variable data . The reason is that both threads can access data at the same time, and thread t1 wants to modify data . Therefore the program has undefined behaviour.

Finally, we have reached the relaxed semantic.

### Relaxed Semantic

The relaxed semantic is the other end of the spectrum. The relaxed semantic is the weakest of all memory models and only guarantees the modification order of atomics. This means all modifications on an atomic occur in some particular total order.

#### No synchronisation and ordering constraints

This is quite easy. If there are no rules, we can not violate them. However, that is too easy; the program should have well-defined behaviour. This means in particular that you typically use synchronisation and ordering constraints of stronger memory orderings to control operations with relaxed semantic. How does this work? A thread can see the effects of another thread in arbitrary order, so you have to make sure there are points in your program where all operations on all threads get synchronised.

A typical example of an atomic operation, in which the sequence of operations doesn’t matter, is a counter. The critical observaion for a counter is not in which order the different threads increment the counter; the critical observation for a counter is that all increments are atomic and that all threads’ tasks are done at the end. Have a look at the following example.

A counter with relaxed semantic

```c++

```

The three most exciting lines are 13, 24, and 26.

In line 13 the atomic number count is incremented using the relaxed semantic, so we have a guarantee that the operation is atomic. The fetch_add operation establishes an ordering on count . The function add (lines 10 - 15) is the work package of the threads. Each thread gets its work package on line 21.

Thread creation is one synchronisation point. The other one being t.join() on line 24.

The creator thread synchronises with all its children in line 24. It waits with the t.join() call until all its children are done. t.join() is the reason that the results of the atomic operations are published. To say it more formally: t.join() is a release operation.

In conclusion, there is a happens-before relation between the increment operation in line 13 and the reading of the counter count in line 26.

The result is that the program always returns 10000. Boring? No, it’s reassuring!

![](../../../images/detail/memory-model/21.png)

A typical example of an atomic counter which uses the relaxed semantic is the reference counter of std::shared_ptr . This only holds for the increment operation. The key property for incrementing the reference counter is that the operation is atomic. The order of the increment operations does not matter. This does not hold for the decrement of the reference counter. These operations need an acquire-release semantic for the destructor.

> 知识点
>
> **The add algorithm is wait-free**
>
> Have a closer look at the function add in line 10. There is no synchronisation involved in the increment operation (line 13). The value 1 is just added to the atomic count 
>
> Therefore, the algorithm is not only lock-free but it is also wait-free.

The fundamental idea of std::atomic_thread_fence is to establish synchronisation and ordering constraints between threads without an atomic operation.