# 原子操作

原子操作是C++内存模型的基础。默认情况下，强内存模型支持原子操作；因此，理解强内存模型的特征很有意义。

##强/弱内存模型

您可能已经从编程协议的小节中了解到：强内存模型中的是顺序一致性，在弱内存模型中的松散语义。

###强内存模型

2004年Java 5.0有了内存模型，2011年C++添加了内存模型。在此之前，Java有一个错误的内存模型，而C++则没有内存模型。多线程编程已经有40~50年的历史了。在1979年，[Leslie Lamport](https://en.wikipedia.org/wiki/Leslie_Lamport) 就定义了顺序一致性的概念。

顺序一致性提供了两个保证:

* 指令按源码顺序执行。
*  线程上的所有操作都遵循一个全局顺序。

在深入研究这两个保证之前，我想强调一下，这些声明只适用于原子操作，但影响并不止于原子操作。

下面图形显示了两个线程。每个线程分别将值存储到变量x或y中，加载另一个变量y或x，并将它们存储在变量res1或res2中。

![](../../../images/detail/memory-model/3.png)

> 两个原子操作

通常，原子操作保证顺序一致性。问题是：这些语句以什么顺序执行?

顺序一致性的第一个保证是，指令按照源代码中定义的顺序执行。这很容易，任何存储操作都无法在加载操作之前进行。

顺序一致性的第二个保证是，所有线程的所有指令必须遵循全局顺序。上图中的情况，意味着线程2看到线程1的操作的顺序与线程1执行它们的顺序相同，这很关键。线程2按照线程1的源代码顺序查看线程1的所有操作，从线程1的角度来看也是如此。你可以将这个特性，想象成一个所有线程都必须遵守的全局时钟。全局时钟就是全局顺序。时钟每发出一次滴答声，就会发生一个原子操作，但你永远不知道是哪个。

我们的谜题还没有解开。我们仍然需要观察，这两个线程交错的执行方式。因此，这两个线程有以下六种交运行方式。

![](../../../images/detail/memory-model/4.png)

> 两条线的六个交错执行可能

很简单，对吧？这就是顺序一致性，也称为**强内存模型**。

### 弱内存模型

我们再参考一下程序开发者和系统之间的“协议”。

这个特殊的例子中，程序开发者使用了原子操作(开发者遵循协议的相应部分)。系统保证程序的行为，从而不会存在数据竞争。另外，系统可以在每个组合中执行四个操作。如果程序开发者使用松散语义，协议的基础部分就会发生巨大的变化。一方面，程序开发者可能很难理解两个线程之间的交错。另一方面，系统有了更多优化的空间。

使用松散语义(也称为弱内存模型)，可使这四种操作有更多的组合。有种很难理解的行为是，线程1可以以不同的顺序查看线程2的操作，这样全局时序图就不存在了。从线程1的角度来看，操作`res2= x.load()`可能在`y.store(1)`之前执行。甚至是，线程1或线程2没有按照源代码中的顺序执行。例如，线程2可以先执行`res2= x.load()`，再执行`y.store(1)`。

“序列一致性”和“松散语义”之间还有一些模型，其中最重要的是“获取-释放”语义。“获取-释放”语义中，程序开发人员需要遵守比“顺序一致性”更弱的规则。相较之下，系统有更多优化的空间。因为线程是在特定同步点上进行同步，所以“获取-释放”语义是理解多线程编程中，同步和部分排序的关键。没有这些同步点，就不可能有(定义良好的)线程、任务或条件变量。

上一节中，介绍了原子操作的默认行为——顺序一致性(为每个原子操作指定内存顺序)。如果没有指定内存顺序，则应用保持顺序一致性，这就意味着`std::memory_order_seq_cst`将默认应用于每个原子操作上。

下面两端代码是等价的：

```c++
x.store(1);
res = x.load();
```

```c++
x.store(1, std::memory_order_seq_cst);
res = x.load(std::memory_order_seq_cst);
```

简单起见，本书使用第一种形式。现在，是深入了解C++内存模型原子性的时候了。就从`std::atomic_flag`开始吧。

## 原子标志

`std::atomic_flag`是原子布尔类型。可以对其进行状态的设置和清除。为了简化说明，我将`clear`状态称为`false`，将`set`状态称为`true`。它的`clear`方法可将其值设置为`false`。使用`test_and_set`方法，可以将值设置回`true`，并返回先前的值。没有方法获取当前值。使用`std::atomic_flag`时，必须使用常量`ATOMIC_FLAG_INIT`将其初始化为`false`。

> 注意：
>
> **ATOMIC_FLAG_INIT**
>
> `std::atomic_flag` 标示需要进行初始化，可以是这样`std::atomic_flag flag = ATOMIC_FLAG_INIT`。不过，不能这样初始化`std::atomic_flag flag(ATOMIC_FLAG_INIT) `。

`std::atomic_flag`有两个突出的特性：

* 唯一的无锁原子类型。程序是系统级别进程的话，那么执行的非阻塞算法就是无锁的。
* 更高级别的线程构建块。

除了`std::atomic_flag`之外，C++标准中的原子内部都会使用互斥锁。这些原子类型有一个`is_lock_free`成员函数，可用来检查原子内部是否使用了互斥锁。在时下流行的微处理器架构上，能得到是“使用了互斥锁”的结果。如果想要无锁编程，那么就要使用该成员函数进行检查，确定是否使用了锁。

> 贴士：
>
> **`std::is_always_lock_free`**
>
> 可以使用`obj.is_lock_free()`，在运行时检查原子类型的实例`obj`是否无锁。在C++17中，可以通过`constexpr`(常量)[`atomic<type>::is_always_lock_free`](https://zh.cppreference.com/w/cpp/atomic/atomic/is_always_lock_free)，在编译时对每个原子类型进行检查，支持该操作的所有硬件实现都无锁时，此检查才返回true。

`std::atomic_flag`的接口非常强大，能够构建自旋锁。自旋锁可以像使用互斥锁一样保护临界区。

> 知识点
>
> **自旋锁**
>
> 自旋锁与互斥锁不同，它并不再等待获取锁。而是，通过频繁地请求锁来获取访问临界区的权利。不过，这会将上下文频繁切换(从用户空间到内核空间)，虽然充分使用了CPU，但也浪费了非常多的时钟周期。线程短时间阻塞时，自旋锁非常有效。通常，会将自旋锁和互斥锁组合着使用。首先，在有限的时间内使用自旋锁，如果不成功，则将线程置于等待(休眠)状态。
>
> 自旋锁不应该在单处理器系统上使用。否则，自旋锁仅是浪费资源，减慢程序处理的速度(最好的情况)，或出现死锁(最坏的情况)。

下面的示例展示了，使用`std::atomic_flag`实现自旋锁

```c++
// spinLock.cpp

#include <atomic>
#include <thread>

class Spinlock{
  std::atomic_flag flag = ATOMIC_FLAG_INIT;
public:
  void lock(){
    while(flag.test_and_set());
  }
  
  void unlock(){
    flag.clear();
  }
};

Spinlock spin;

void workOnResource(){
  spin.lock();
  // shared resource
  spin.unlock();
}

int main(){
  std::thread t(workOnResource);
  std::thread t2(workOnResource);
  
  t.join();
  t2.join();
}
```

线程t和t2(第31行和第32行)在争夺临界区的访问权。简单起见，第24行只有一条注释。自旋锁是如何工作的呢？自旋锁也有上锁和解锁。

当线程t执行函数`workOnResource`时，可能会发生以下情况。

1.  因为锁成功获取，所以线程`t`获取锁。若第11行的标志初始值为false，则锁调用成功。这种情况下，线程`t`的原子操作将其设置为true。所以，当`t`线程获取锁后，true将会让while陷入不停的循环，使得线程`t2`陷入了激烈的竞争当中。线程`t2`不能将标志设置为false，因此`t2`必须等待，直到线程`t1`执行`unlock`(解锁)并将标志设置为false(第14 - 16行)。
2.  线程`t`没有得到锁时，情况1中的`t2`一样，需要等待。

我们将注意力放在`std::atomic_flag`的`test_and_set`成员函数上。`test_and_set`函数包含两个操作：读和写。原子操作就是对这两种操作进行限制的。如果没有限制，我们将对共享资源同时进行读和写操作(第24行)。根据定义，这就是“数据竞争”，程序有产生未定义的行为。

将自旋锁的主动等待和互斥锁的被动等待做一下比较，应该会非常有趣。

### 自旋锁 vs. 互斥锁

如果函数`workOnResource`在第24行停顿2秒，那CPU负载会发生怎样的变化?

```c++
// spinLockSleep.cpp

#include <atomic>
#include <thread>

class Spinlock{
  std::atomic_flag flag = ATOMIC_FLAG_INIT;
public:
  
  void lock(){
    while(flag.test_and_set());
  }
  
  void unlock(){
    flag.clear();
  }
  
};

Spinlock spin;

void workOnResource(){
  spin.lock();
  std::this_thread::sleep_for(std::chrono::milliseconds(2000));
  spin.unlock();
}

int main(){
  
  std::thread t(workOnResource);
  std::thread t2(workOnResource);
  
  t.join();
  t2.join();
  
}
```

如下图所示，每次四个核中的一个是跑满了的。

![](../../../images/detail/memory-model/5.png)

我的PC上有一个核的负载达到100%，每次不同的核心执行”忙等待“。

我现在用互斥锁来替换自旋锁。让我们看下会发生什么。

```c++
// mutex.cpp

#include <mutex>
#include <thread>

std::mutex mut;

void workOnResource(){
  mut.lock();
  std::this_thread::sleep_for(std::chrono::milliseconds(5000));
  mut.unlock();
}

int main(){
  
  std::thread t(workOnResource);
  std::thread t2(workOnResource);
  
  t.join();
  t2.join();
}
```

虽然执行了好几次，但是并没有观察到任何一个核上有显著的负载。

这样就能看出二者间的区别了吧。

![](../../../images/detail/memory-model/6.png)

接下来，让我们了解下高级的`std::atomic`模板。

##  `std::atomic`模板

`std::atomic`的有各种各样的变体。

直接使用模板类：`std::atomic<bool>`和`std::atomic<user-defined type>`。

部分特化可用于指针类：`std::atomic<t*>`。

完全特化只能用于整型：`std::atomic<integral type>`。

布尔原子类型和用户定义原子类型具有相同的接口，原子指针扩展了布尔原子类型，以及整数原子类型的接口。因其扩展了原子指针的接口，所以同样适用于整数原子类型。

不过，不保证`std::atomic`的各种变体都是无锁的。

让我们从最简单的`std::atomic<bool>`开始说起吧。

### `std::atomic<bool>`

`std::atomic<bool>`的功能比`std::atomic_flag`强大很多。并且，可以显式地将其设置为true或false。

> 注意：
>
> **原子类型不可为volatile**
>
> C#和Java中的关键字`volatile`与C++中的关键字`volatile`的区别，这是`volatile`关键字和`std::atomic`之间的区别。
>
> * `volatile`：表示不允许对特定的对象进行读写优化。
> * `std::atomic`：用来定义线程安全的原子变量。
>
> 关键字`volatile`在Java和C#中，与`std::atomic`在C++中的含义相同。另外，在C++多线程语义中，没有`volatile`。
>
> `volatile`多嵌入式编程中，表示可以(独立于常规程序流)进行更改的对象，例如：表示外部设备的对象(内存映射I/O)。由于这些对象可以(独立于常规程序流)进行更改，并且直接写入主内存，因此不会在缓存中进行优化存储。

这对于同步两个线程已经足够了，可以用`std::atomic<bool>`实现一种条件变量。

因此，我们先要使用条件变量。

```c++
// conditionVariable.cpp

#include <condition_variable>
#include <iostream>
#include <thread>
#include <vector>

std::vector<int> mySharedWork;
std::mutex mutex_;
std::condition_variable condVar;

bool dataReady{false};

void waitingForWork(){
  std::cout << "Waiting " << std::endl;
  std::unique_lock<std::mutex> lck(mutex_);
  condVar.wait(lck, []{return dataReady;});
  mySharedWork[1] = 2;
  std::cout << "Work done " << std::endl;
}

void setDataReady(){
  mySharedWork = {1, 0, 3};
  {
    std::lock_guard<std::mutex> lck(mutex_);
    dataReady = true;
  }
  std::cout << "Data prepared" << std::endl;
  condVar.notify_one();
}

int main(){
  std:cout << std::endl;
  
  std::thread t1(waitingForWork);
  std::thread t2(setDataReady);
  
  t1.join();
  t2.join();
  
  for (auto v : mySharedWork){
    std::cout << v << " ";
  }
  
  std::cout << "\n\n";
}
```

简单说一下这段代码。如要深入讨论条件变量，请阅读本书有关条件变量的章节。

线程t1在(第17行)等待线程t2的通知。两个线程使用相同的条件变量`condVar`，并在同一个互斥锁上进行同步。工作流如下所示：

* 线程t1
  * 当获取锁lck时，等待数据准备好的通知 `condVar.wait(lck, []{ return dataReady; })` 。
  * 在得到通知后，执行`mySharedWork[1] = 2`。

* 线程t2
  * 准备数据`mySharedWork = {1, 0, 3}`
  * 将非原子布尔类型的`dataReady`置为true。
  * 通过`condVar.notify_one`发布通知。

线程t2将`dataReady`设置为true，线程t1使用lambda对`dataReady`进行检查。不过，条件变量可能会出现两种不好的情况:

1. 伪唤醒：接受者在没有收到通知时被唤醒。
2. 超前唤醒：接收方在未处于等待状态时获得通知。

现在使用`std::atomic<bool> `进行实现：

```c++
// atomicCondition.cpp

#include <atomic>
#include <chrono>
#include <iostream>
#include <thread>
#include <vector>

std::vector<int> mySharedWork;
std::atomic<bool> dataReady(false);

void waitingForWork(){
  std::cout << "Waiting " << std::endl;
  while(!dataReady.load()){
    std::this_thread::sleep_for(std::chrono::milliseconds(5)); 
  }
  mySharedWork[1] = 2;
  std::cout << "Work done " << std::endl;
}

void setDataReady(){
  mySharedWork = {1,0,3};
  dataReady = true;
  std::cout << "Data prepared" << std::endl;
}

int main(){
  
  std::cout << std::endl;
  
  std::thread t1(waitingForWork);
  std::thread t2(setDataReady);
  
  t1.join();
  t2.join();
  
  for (auto v : mySharedWork){
    std::cout << v << " "; 
  }
  
  std::cout << "\n\n";
}
```

如何保证第17行在第14行之后执行？或者说，线程t1在线程t2执行`mySharedWork ={1,0,3}`(第22行)后执行`mySharedWork[1] = 2`(第17行)。

* 22行先于23行执行。
* 14行先于17行执行。
* 14 23行与14行同步
* 因为同步建立了先行关系，并且先行关系可以传递，所以`mySharedWork = {1,0,3}`先于`mySharedWork[1] = 2`执行。

很容易理解，对吧？为了简单，忽略了同步创建的线程间的先行关系，以及线程间的先行建立的先行关系。如果你对这里的细节感兴趣，可以参考这里进行延伸阅读：[内存序(memory_order)](http://en.cppreference.com/w/cpp/atomic/memory_order)。

明确一下关键点：使用条件变量`condVar`或原子类型对共享变量`mySharedWork`的访问进行保护。尽管，`mySharedWork`本身不受锁或原子的保护，但当前的方式也是可行的。

两段程序产生了相同的结果。

![](../../../images/detail/memory-model/7.png)

> 知识点
>
> **推拉原理**
>
> 其实这里还有些不同。具有条件变量线程的同步与`std::atomic<bool>`之间有一个关键性的区别。条件变量通知等待的线程(`condVar.notify()`)，让其继续工作。使用检查`std::atomic<bool>`的等待线程，只是为了确定发送方是否完成了其工作(`dataRead = true`)。
>
> 条件变量通知等待线程对应"推原则(push principle)"，而原子布尔值的重复轮询值对"拉原则(pull principle)"。

`std::atomic<bool>`和`std::atomic`的其他全/部分特化都支持的原子操作：`compare_exchange_strong`和`compare_exchange_strong`。

> 知识点
>
> **compare_exchange_strong和compare_exchange_weak** 
>
> compare_exchange_strong has the syntax: bool compare_exchange_strong(T& expected, T& desired) . Because this operation compares and exchanges its values in one atomic operation, it is often called compare and swap (CAS). This kind of operation is available in many programming languages and is the foundation of non-blocking algorithms. Of course, the behaviour may vary a little. atomicValue.compare_exchange_strong(expected, desired) has the following behaviour.
>
> *  If the atomic comparison of atomicValue with expected returns true , atomicValue is set in the same atomic operation to desired 
> * If the comparison returns false , expected is set to atomicValue 
>
> The reason the operation compare_exchange_strong is called strong is apparent. There is also a method compare_exchange_weak . The weak version can fail spuriously. That means, although *atomicValue == expected holds, atomicValue was not set to desired and the function call returns false , so you have to check the condition in a loop: while (!atomicValue.compare_exchange_weak(expected, desired)) . The weak form exists because some processor doesn’t support an atomic compare-exchange instruction. When called in a loop the weak form should be preferred. On some platforms, the weak form can run faster.
>
> CAS operations are open for the so-called ABA problem. This means you read a value twice and each time it returns the same value A; therefore you conclude that nothing changed in between. However, you overlooked that the value may have changed to B in between readings.

The weak forms (1-2) of the functions are allowed to fail spuriously, that is, act as if *this != expected even if they are equal. When a compare-and-exchange is in a loop, the weak version may has better performance on some platforms.

In addition to booleans, there are atomics for pointers, integrals and user-defined types. The rules for user-defined types are unique.

All variations of std::atomic support the CAS operations.

### User Defined Atomics `std::atomic<user-defined type>`

Thanks to the class template std::atomic , you can define your user-defined atomic type.

There are a lot of strong restrictions on a user-defined type if you use it for an atomic type std::atomic<user-defined type> . The atomic type std::atomic<user-defined type> supports the same interface as std::atomic<bool> 

Here are the restrictions for a user-defined type to become an atomic type:

* The copy assignment operator for user-defined type, for all base classes of the user-defined type and all non-static members of the user-defined type, must be trivial. This means that you must not define the copy assignment operator, but you can request it from the compiler by
  using [default](http://en.cppreference.com/w/cpp/keyword/default)
* a user-defined type must not have virtual methods or virtual base classes
* auser-defined type must be bitwise comparable so that the C functions [memcpy](http://en.cppreference.com/w/cpp/string/byte/memcpy) or [memcmp]( http://en.cppreference.com/w/cpp/string/byte/memcmp) can be applied

Most popular platforms can use atomic operations for std::atomic<user-defined type> if the size of the user-defined type is not bigger as the size of an int 

> 知识点
>
> **Check the type properties at compile time**
>
> The type properties on a user-defined type can be checked at compile time, by using the following functions: std::is_trivially_copy_constructible , std::is_polymorphic and std::is_trivial . All these functions are part of the very powerful [type-traits library]( http://en.cppreference.com/w/cpp/header/type_traits).

**std::atomic<T*>**

std::atomic<T*> is a partial specialisation of the class template std::atomic . The atomic pointer std::atomic<T*> supports all methods that std::atomic<bool> or std::atomic<user-defined type> support. It behaves like a plain pointer T* . std::atomic<T*> supports pointer arithmetic and pre- and post-increment or pre- and post-decrement operations.

Have a look at the short example.

```c++
int intArray[5];
std::atomic<int*> p(intArray);
p++;
assert(p.load() == &intArray[1]);
p+=1;
assert(p.load() == &intArray[2]);
--p;
assert(p.load() == &intArray[1]);
```

In C++11 there are atomic types for the known integral data types.

**std::atomic<integral type>**

For each integral type there is a full specialisation std::atomic<integral type> of std::atomic . An std::atomic<integral type> supports all operations that std::atomic<T*> support, and more. First of all. Which specialisations for integral types exists? Here are the details:

* character types: char , char16_t , char32_t , and wchar_t
* standard signed integer types: signed char , short , int , long , and long long
*  standard unsigned integer types: unsigned char , unsigned short , unsigned int , unsigned long , and unsigned long long
*  additional integer types, defined in the header [<cstdint>](http://en.cppreference.com/w/cpp/header/cstdint)
  * int8_t , int16_t , int32_t , and int64_t (signed integer with exactly 8, 16, 32, and 64 bits)
  * uint8_t , uint16_t , uint32_t , and uint64_t (unsigned integer with exactly 8, 16, 32, and 64 bits)
  * int_fast8_t , int_fast16_t , int_fast32_t , and int_fast64_t (fastest signed integer with at least 8, 16, 32, and 64 bits)
  * uint_fast8_t , uint_fast16_t , uint_fast32_t , and uint_fast64_t (fastest unsigned integer with at least 8, 16, 32, and 64 bits)
  * int_least8_t , int_least16_t , int_least32_t , and int_least64_t (smallest signed integer with at least 8, 16, 32, and 64 bits)
  * uint_least8_t , uint_least16_t , uint_least32_t ,and uint_least64_t (smallest unsigned integer with at least 8, 16, 32, and 64 bits)
  * intmax_t , and uintmax_t (maximum signed and unsigned integer)
  * intptr_t , and uintptr_t (signed and unsigned integer for holding a pointer)

std::atomic<integral type> supports the composite assignment operators += , -= , &= , |= and ^= and their fetch pedants: fetch_add , fetch_sub , fetch_and , fetch_or and fetch_xor . There is a small difference in the composite assignment and the fetch version. The composite assignment operators return the new value; the fetch variations returns the old value. Additionally, the pre- and post-increment and pre- and post-decrement ( ++x , x++ , --x , and x-- ) are available.

A more in-depth look provides more insight: there is no atomic multiplication, atomic division, nor atomic shift operation available. This is not a significant limitation, because these operations are seldom needed and can easily be implemented. Here is an example of an atomic fetch_mult function.

An atomic multiplication with compare_exchange_strong

```c++

```

One point worth mentioning is that the multiplication in line 9 only happens if the relation oldValue == shared holds. I put the multiplication in a while loop to be sure that the multiplication always take place because there are two instructions for the reading of oldValue in line 8 and its usage in line 9. Here is the result of the atomic multiplication.

![](../../../images/detail/memory-model/8.png)

> 知识点
>
> **The fetch_mult algorithm is lock_free**
>
> The algorithm fetch_mult (line 6) multiplies std::atomic shared by mult . The key observation is that there is a small window of time between the reading of the old value T oldValue = shared Load (line 8) and the comparison with the new value in line 9. Therefore an other thread can always step in and change oldValue .If you think about a bad interleaving of threads, you see that there is no per-thread progress guarantee.
>
> The consequence is that the algorithm is lock-free, but not wait-free.

### Type Aliases

For all std::atomic<bool> and all std::atomic<integral type> the C++ standard provide type aliases if the integral type is available.

Type aliases for std::atomic<bool> and std::atomic<integral type>



### All Atomic Operations

First, here is the list of all operations on atomics.



The atomic types have no copy constructor or copy assignment operator but the support assignment from and implicit conversion to the underlying built-in type. The composite assignment operators return the new value; the fetch variations returns the old value. The composite assignment operators return values and not references to the assigned object.

Implicit conversion to the underlying type

```c++
std::atomic<long long> atomOb(2011j;
atomObj = 2014;
long long nonAtomObj = atomObj;
```

Each method supports an additional memory-ordering argument. The default for the memory-ordering argument is std::memory_order_seq_cst but you can also use std::memory_order_relaxed , std::memory_order_consume , std::memory_order_acquire , std::memory_order_release , or std::memory_order_acq_rel .The compare_exchange_strong and compare_exchange_weak method can be parametrised with two memory-orderings. One for the success and one for the failure case.

If you only explicitly provide one memory-ordering, it is used for the success and the failure case. Here are the details to memory-ordering.

Of course, not all operations are available on all atomic types. The table shows the list of all atomic operations depending on the atomic types.

All atomic operations depeding on the atomic type



### Free Atomic Functions

The functionality of the flag std::atomic_flag and the class template std::atomic can also be used with free functions. Because these functions use pointers instead of references they are compatible with C. The atomic free functions are available for the std::atomic_flag and the types such as the types you can use with the class template std::atomic 

The free functions for a std::atomic_flag are called: std::atomic_clear() , std::atomic_clear_explicit , std::atomic_flag_test_and_set() , and std::atomic_flag_test_set_explicit() . The first argument of all four variations is a pointer to a std::atomic_flag . Additionally, the two _exclicit variations expect a memory-ordering.

For each std::atomic type their is a corresponding free function available. They free functions follow a straightforward naming convention: add just the prefix atomic_ in front of it. For example, a method call at.store() on a std::atomic becomes std::atomic_store() , and std::atomic_store_except() . The first overload expects in this case a pointer and the second overload an memory-ordering.

For completeness, here are all overloads: [atomic]( http://en.cppreference.com/w/cpp/atomic)

With one exception, free functions are only available on atomic types. The prominent exception to this rule is std::shared_ptr 

### std::shared_ptr

std::shared_ptr is the only non-atomic data type on which you can apply atomic operations. First, let me write about the motivation for this exception.

The C++ committee saw the necessity that instances of smart pointers should provide a minimum atomicity guarantee in multithreading programs. What is the meaning of the minimal atomicity guarantee for std::shared_ptr ? The control block of the std::shared_ptr is thread-safe. This means that the increase and decrease operations of the reference-counter are atomic. You also have the guarantee that the resource is destroyed exactly once.

The assertion that a std::shared_ptr provides, are described by [Boost](http://www.boost.org/doc/libs/1_57_0/libs/smart_ptr/shared_ptr.htm#ThreadSafety)

1. A shared_ptr instance can be “read” (accessed using only const operations) simultaneously by multiple threads.
2. Different shared_ptr instances can be “written to” (accessed using mutable operations such as operator= or reset) simultaneously by multiple threads (even when these instances are copies, and share the same reference count underneath).

To make the two statements clear, let me show a simple example. When you copya std::shared_ptr in a thread, all is fine.

Thread-safe copying of a std::shared_ptr

```c++
std::shared_ptr<int> ptr = std::make_shared<int>(2011);

for (auto i = 0; i < 10; i++){
  std::thread([ptr]{
    std::shared_ptr<int> localPtr(ptr);
    localPtr = std::make_shared<int>(2014);
  }).detach();
}
```

Let’s first look at line 5. By using copy construction for the std::shared_ptr localPtr , only the control block is used. This is thread-safe. Line 6 is a little bit more interesting. The localPtr is set to a new std::shared_ptr . This is not a problem from the multithreading point of view: the lambda-function (line 4) binds ptr by copy. Therefore, the modification of localPtr takes place on a copy.

The story changes dramatically if I get the std::shared_ptr by reference.

A data race on a std::shared_ptr

```c++

```

The lambda-function binds the std::shared_ptr ptr in line 4 by reference. This means, the assignment (line 5) may become a concurrent reading and writing of the underlying resource; therefore, the program has undefined behaviour.

Admittedly that last example was not very easy to achieve. std::shared_ptr requires special attention in a multithreading environment. std::shared_ptr is the only non-atomic data type in C+ for which atomic operations exist.

### Atomic Operations on std::shared_ptr

There are specialisations for the atomic operations load , store , compare_and_exchange for a std::shared_ptr . By using the explicit variant you can even specify the memory-ordering. Here are the free atomic operations for std::shared_ptr 

Atomic operations for std::shared_ptr



For the details, have a look at [cppreference.com]( http://en.cppreference.com/w/cpp/memory/shared_ptr). Now it is quite easy to modify a shared pointer that is bound by reference in a thread-safe way.

A data race for a std::shared_ptr resolved

```c++

```

The update of the std::shared_ptr ptr in the expression auto localPtr = std::make_shared<int>(2014) is thread-safe. All is well? NO! Finally, we need atomic smart pointers.

> 知识点
>
> That is not the end of the story for atomic smart pointers. With C++20 we can expect with high probability two new smart pointers: std::atomic<std::shared_ptr> and std::atomic<std::weak_ptr> . For the impatient reader here are the details of the upcoming atomic smart pointers.

Atomics and their atomic operations are the basic building blocks for the memory model. They establish synchronisation and ordering constraints that hold for both atomics and non-atomics. Let’s have a more in-depth look into the synchronisation and ordering constraints.



